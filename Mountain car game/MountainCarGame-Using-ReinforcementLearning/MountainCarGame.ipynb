{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aayu-7/MountainCarGame-Using-ReinforcementLearning/blob/main/MountainCarGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vZ74GdYS7gT"
   },
   "source": [
    " **Problem Description**\n",
    "\n",
    "In the MountainCar environment, a car starts at the bottom of a valley. The agent must learn to drive up a steep hill to reach the flag. Since the carâ€™s engine is not powerful enough to climb the hill directly, it must first build momentum by driving back and forth.\n",
    "The car's engine isn't powerful enough to drive directly up the hill. Instead, the agent must learn to build momentum by first moving left to gain speed and then driving right to reach the goal.\n",
    "\n",
    "    State Space: Continuous values for position and velocity.\n",
    "    Action Space: Three discrete actions:\n",
    "        0: Push left\n",
    "        1: No push\n",
    "        2: Push right\n",
    "    Reward: The agent gets -1 for each time step until it reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OMhh8fUTc-G"
   },
   "source": [
    "Training:\n",
    "\n",
    "    The agent is trained over 5000 episodes.\n",
    "    Epsilon-greedy policy ensures exploration initially, which decays over time to favor exploitation.\n",
    "\n",
    "Testing:\n",
    "\n",
    "    The trained agent is evaluated and its performance recorded as a video.\n",
    "\n",
    "Video Display:\n",
    "\n",
    "    The RecordVideo wrapper saves the video, and it is displayed using HTML in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R1pDBWRnRcEy",
    "outputId": "77437ff5-d306-4edd-f316-601a3a025d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 524.3/721.7 kB 5.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 3.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (1.24.3)\n",
      "Collecting cloudpickle>=1.2.0 (from gym)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827633 sha256=5ec63eb4e3c846ecc801ef5c1a79aa525e48d7133e34e564a21b61cd8556af45\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-3.1.0 gym-0.26.2 gym_notices-0.0.8\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install gym\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "# Install the required library\n",
    "!pip install gym\n",
    "\n",
    "# Import required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import glob\n",
    "import io\n",
    "from base64 import b64encode\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# Function to discretize state space\n",
    "def discretize_state(state, bins):\n",
    "    position_bins = np.linspace(-1.2, 0.6, bins)  # Discretize position\n",
    "    velocity_bins = np.linspace(-0.07, 0.07, bins)  # Discretize velocity\n",
    "    position_idx = np.digitize(state[0], position_bins) - 1\n",
    "    velocity_idx = np.digitize(state[1], velocity_bins) - 1\n",
    "    return (position_idx, velocity_idx)\n",
    "\n",
    "# Function to display video\n",
    "def show_video():\n",
    "    video_path = glob.glob('./video/*.mp4')[0]\n",
    "    video = io.open(video_path, 'r+b').read()\n",
    "    encoded = b64encode(video)\n",
    "    return HTML(data=f'''\n",
    "        <video width=\"640\" height=\"480\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\">\n",
    "        </video>''')\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_bins = 20  # Number of bins for discretization\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "n_episodes = 5000\n",
    "\n",
    "# Initialize Q-table\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((n_bins, n_bins, n_actions))\n",
    "\n",
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "    state = discretize_state(env.reset(), n_bins)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action: exploration vs exploitation\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        # Perform action\n",
    "        next_state_raw, reward, done, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state_raw, n_bins)\n",
    "\n",
    "        # Update Q-value\n",
    "        best_next_action = np.argmax(q_table[next_state])\n",
    "        td_target = reward + gamma * q_table[next_state][best_next_action]\n",
    "        q_table[state][action] += alpha * (td_target - q_table[state][action])\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n",
    "# Wrap environment for recording\n",
    "env = RecordVideo(env, \"./video\", episode_trigger=lambda x: True)\n",
    "\n",
    "# Test the trained agent\n",
    "state = discretize_state(env.reset(), n_bins)\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    next_state_raw, _, done, _ = env.step(action)\n",
    "    state = discretize_state(next_state_raw, n_bins)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Display the video\n",
    "show_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRMVFBNFSMN7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvGfGVIgDiP4Nz+Mkxrqtm",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
